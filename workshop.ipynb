{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Build Your Own Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Featuring Tensorflow (TFlow).\n",
    "\n",
    "We'll be going classifying MNIST data, which is a set of ~70,000 images of handwritten digits. Bear in mind, this is a solved problem, so we're not doing anything novel.\n",
    "\n",
    "---\n",
    "**What you should leave with:**\n",
    "You should leave here with a practical understanding of how to implement an Artificial Neural Network (ANN) from nothing. The concepts don't change when you move to different domains, simply the way in which you apply them. Your understanding of the *central* concept of ANNs, **backpropagation (backprop)** should be well founded and given some more practice, you could explain this to a friend.\n",
    "\n",
    "You should also leave here with a minimal understanding TensorFlow and how using such a library can speed up your model development, as well as understanding some of it's drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "1. [Some Pre-processing](#1.-Some-Pre-processing)\n",
    "2. [Building an ANN from Sratch](#2.-Building-an-ANN-from-Sratch)\n",
    "3. [Rebuilding the ANN in TensorFlow](#3.-Rebuilding-the-ANN-in-TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Some Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:32:54.466913Z",
     "start_time": "2017-10-02T20:32:40.592018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.694620Z",
     "start_time": "2017-10-02T20:32:54.470018Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def convert(imgf, labelf, outf, n):\n",
    "    f = gzip.open(imgf, \"rb\")\n",
    "    o = open(outf, \"w\")\n",
    "    l = gzip.open(labelf, \"rb\")\n",
    "\n",
    "    f.read(16)\n",
    "    l.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n):\n",
    "        image = [ord(l.read(1))]\n",
    "        for j in range(28*28):\n",
    "            image.append(ord(f.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    for image in images:\n",
    "        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "    f.close()\n",
    "    o.close()\n",
    "    l.close()\n",
    "\n",
    "convert(\"data/train-images-idx3-ubyte.gz\", \"data/train-labels-idx1-ubyte.gz\", \"data/mnist_train.csv\", 60000)\n",
    "convert(\"data/t10k-images-idx3-ubyte.gz\",  \"data/t10k-labels-idx1-ubyte.gz\",  \"data/mnist_test.csv\",  10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building an ANN from Sratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build an Artificial Neural Network (ANN) from the ground-up, using raw [Python](http://python.org/), [NumPy](http://numpy.org/), and [ScipPy](http://scipy.org/).\n",
    "\n",
    "We'll build the ANN from the group up to give you the intuition behind how one would go about creating an ANN. These **_can_** run faster than the ANN built with TensorFlow, Torch, or other libraries, but these libraries introduce simplicity of building (which you'll see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### On to the workshop.\n",
    "\n",
    "Let's import our dependencies, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.709003Z",
     "start_time": "2017-10-02T20:34:42.699622Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a ANN class, called `NeuralNetwork`, this will contain two functions, and an initializer.\n",
    "\n",
    "The functions are: `train(...)` and `query(...)`. The `...` is because we don't necessarily know what we should be passing through to these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We're going to build the functions that go into the class, so we take things a step as a time, and so this commentary can be there in between. Once we've built the functions, we'll copy-paste them into the class definition and run with it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building the Initializer: `__init__(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__(...)` is almost like a constructor. Essentially, we use this to setup some instance variables that enable us to avoid passing the ANN's configuration to each function we call.\n",
    "\n",
    "This function should have a few variables that keep track of and add to the class:\n",
    "- the number of input nodes\n",
    "- the number of hidden nodes\n",
    "- the number of output nodes\n",
    "- the learning rate\n",
    "- the weights from the input to hidden layers\n",
    "- the weights from the hidden to output layers\n",
    "- the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.853783Z",
     "start_time": "2017-10-02T20:34:42.717344Z"
    }
   },
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def __init__(self, n_inodes, n_hnodes, n_onodes, learn_rate):\n",
    "    ## these determine the number of nodes per layer\n",
    "    self.i_cnt = n_inodes\n",
    "    self.h_cnt = n_hnodes\n",
    "    self.o_cnt = n_onodes\n",
    "    \n",
    "    ## specify the learning rate\n",
    "    self.lr = learn_rate\n",
    "    \n",
    "    \n",
    "    ## weight initialization\n",
    "    ## this can be done in one of two ways;\n",
    "    ## 1. we can randomly do so, then shift by 0.5 to 0-center our weights to introduce some negativity\n",
    "    ## 2. we can pull from a normal distribution based on some rather well established rationales\n",
    "    \n",
    "    ## going with option 1:\n",
    "    self.w_i2h = np.random.rand(self.h_cnt, self.i_cnt) - 0.5\n",
    "    self.w_h2o = np.random.rand(self.o_cnt, self.h_cnt) - 0.5\n",
    "    \n",
    "    ## going with option 2:\n",
    "    self.w_i2h = np.random.normal(0, pow(self.h_cnt, -0.5), (self.h_cnt, self.i_cnt))\n",
    "    self.w_h2o = np.random.normal(0, pow(self.o_cnt, -0.5), (self.o_cnt, self.h_cnt))\n",
    "    \n",
    "    \n",
    "    ## we can now specify the activation function, we'll do so as a lambda function\n",
    "    self.activation = lambda x: expit(x)\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building the Query Function: `query(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `query(...)` function should enable us to talk to the ANN and ask it to classify some images we hand it.\n",
    "\n",
    "We write this function before `train(...)` because it's less complex in nature because it equates to a forward pass through the network. This should help us ground the ideas that we should be implementing into `train(...)` when we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.921291Z",
     "start_time": "2017-10-02T20:34:42.857016Z"
    }
   },
   "outputs": [],
   "source": [
    "def query():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def query(self, input_list):\n",
    "    ## convert input list to np.array and transpose because of matrix mult\n",
    "    inputs = np.array(input_list, ndmin=2).T\n",
    "    \n",
    "    \n",
    "    ## propagate the input through the hidden layer\n",
    "    ### recall at X_{hidden} = W_{input_hidden} * I_{inputs}\n",
    "    hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "    ### pass `hidden_in` through the activation function to calculate the output\n",
    "    hidden_out = self.activation(hidden_in)\n",
    "    \n",
    "    \n",
    "    ## propagate the hidden output through the ouput layer\n",
    "    ### recall at X_{output} = W_{hidden_output} * I_{hidden_out}\n",
    "    output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "    ### pass `output_in` through the activation function to calculate the output\n",
    "    output_out = self.activation(output_in)\n",
    "    \n",
    "    return output_out\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building the Train Function: `train(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train(...)` function is how the ANN learns. We'll hand it our dataset with the labels for it to validate itself on by completing forward passes and updating the weights through backprop.\n",
    "\n",
    "We'll need to hand this function:\n",
    "- our inputs\n",
    "- our expected values\n",
    "\n",
    "Now that we've given our network data to train on, we need to implement the forward pass, followed by the backward pass. **Recall** that the backward pass involves a few stages. First, we need to calculate the output error, then distribute that error backwards across the network. This will update our weights, but the update will be moderated by the learning rate, which we specified earlier in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.995363Z",
     "start_time": "2017-10-02T20:34:42.925721Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def train(self, input_list, target_list):\n",
    "    ## convert input list to np.array and transpose because of matrix mult\n",
    "    inputs  = np.array(input_list, ndmin=2).T\n",
    "    targets = np.array(target_list, ndmin=2).T\n",
    "    \n",
    "    \n",
    "    hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "    hidden_out = self.activation(hidden_in)\n",
    "    \n",
    "    output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "    output_out = self.activation(output_in)\n",
    "    \n",
    "    output_err = (target - output_out)\n",
    "    \n",
    "    hidden_err = np.dot(self.w_h2o.T, output_err)\n",
    "    \n",
    "    self.w_h2o += self.lr * np.dot((output_err * output_out * (1 - output_out)), np.transpose(hidden_out))\n",
    "    self.w_i2h += self.lr * np.dot((hidden_err * hidden_out * (1 - hidden_out)), np.transpose(inputs))\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assembling the `NeuralNetwork` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:43.171642Z",
     "start_time": "2017-10-02T20:34:43.001125Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, n_inodes, n_hnodes, n_onodes, learn_rate):\n",
    "        ## these determine the number of nodes per layer\n",
    "        self.i_cnt = n_inodes\n",
    "        self.h_cnt = n_hnodes\n",
    "        self.o_cnt = n_onodes\n",
    "\n",
    "        ## specify the learning rate\n",
    "        self.lr = learn_rate\n",
    "\n",
    "\n",
    "        ## weight initialization\n",
    "        ## this can be done in one of two ways;\n",
    "        ## 1. we can randomly do so, then shift by 0.5 to 0-center our weights to introduce some negativity\n",
    "        ## 2. we can pull from a normal distribution based on some rather well established rationales\n",
    "\n",
    "        ## going with option 1:\n",
    "        self.w_i2h = np.random.rand(self.h_cnt, self.i_cnt) - 0.5\n",
    "        self.w_h2o = np.random.rand(self.o_cnt, self.h_cnt) - 0.5\n",
    "\n",
    "        ## going with option 2:\n",
    "        self.w_i2h = np.random.normal(0, pow(self.h_cnt, -0.5), (self.h_cnt, self.i_cnt))\n",
    "        self.w_h2o = np.random.normal(0, pow(self.o_cnt, -0.5), (self.o_cnt, self.h_cnt))\n",
    "\n",
    "\n",
    "        ## we can now specify the activation function, we'll do so as a lambda function\n",
    "        self.activation = lambda x: expit(x)\n",
    "    \n",
    "    def train(self, input_list, target_list):\n",
    "        ## convert input list to np.array and transpose because of matrix mult\n",
    "        inputs  = np.array(input_list, ndmin=2).T\n",
    "        targets = np.array(target_list, ndmin=2).T\n",
    "\n",
    "\n",
    "        hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "\n",
    "        output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "        output_out = self.activation(output_in)\n",
    "\n",
    "        output_err = (targets - output_out)\n",
    "\n",
    "        hidden_err = np.dot(self.w_h2o.T, output_err)\n",
    "\n",
    "        self.w_h2o += self.lr * np.dot((output_err * output_out * (1 - output_out)), np.transpose(hidden_out))\n",
    "        self.w_i2h += self.lr * np.dot((hidden_err * hidden_out * (1 - hidden_out)), np.transpose(inputs))\n",
    "        \n",
    "    def query(self, input_list):\n",
    "        ## convert input list to np.array and transpose because of matrix mult\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "\n",
    "\n",
    "        ## propagate the input through the hidden layer\n",
    "        ### recall at X_{hidden} = W_{input_hidden} * I_{inputs}\n",
    "        hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "        ### pass `hidden_in` through the activation function to calculate the output\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "\n",
    "\n",
    "        ## propagate the hidden output through the ouput layer\n",
    "        ### recall at X_{output} = W_{hidden_output} * I_{hidden_out}\n",
    "        output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "        ### pass `output_in` through the activation function to calculate the output\n",
    "        output_out = self.activation(output_in)\n",
    "\n",
    "        return output_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training the Network on MNIST\n",
    "\n",
    "Now we'll move on the training the network on MNIST, but to do so, we need to specify some parameters.\n",
    "\n",
    "**Recall** that these images are `28x28` pixel images, which results in a total of `784` inputs. We ultimately need to classify these images into `10` classes, as we're analyzing the numbers `0-9`. The hidden layers is rather arbitrary in size, so we can use just about any amount of hidden layers we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:43.269719Z",
     "start_time": "2017-10-02T20:34:43.174403Z"
    }
   },
   "outputs": [],
   "source": [
    "n_inodes = 1\n",
    "n_hnodes = 1\n",
    "n_onodes = 1\n",
    "\n",
    "learn_rt = 1\n",
    "\n",
    "nn = NeuralNetwork(n_inodes, n_hnodes, n_onodes, learn_rt)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "n_inodes = 784\n",
    "n_hnodes = 200\n",
    "n_onodes =  10\n",
    "\n",
    "learn_rt = 0.1\n",
    "\n",
    "nn = NeuralNetwork(n_inodes, n_hnodes, n_onodes, learn_rt)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've initialized the ANN, so now we need to actually execute the training of it. We'll train over `N` *epochs*, which are essentially just the number of times we go over the data to see if we can continue to refine the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:42:42.093456Z",
     "start_time": "2017-10-02T20:40:03.720358Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    for record, label in zip(mnist.train.images, mnist.train.labels):\n",
    "        inputs = record * 0.99 + 0.01\n",
    "        targets = label * 0.98 + 0.01\n",
    "        nn.train(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the Network\n",
    "\n",
    "We've built and trained the ANN, now; so the next step we should take is to test our accuracy to see if our model has actually learned, well, from the data we've given it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:19.354753Z",
     "start_time": "2017-10-02T20:37:19.233723Z"
    }
   },
   "outputs": [],
   "source": [
    "test_file = open(\"data/mnist_test.csv\", \"r\")\n",
    "test_list = test_file.readlines()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:46:10.950397Z",
     "start_time": "2017-10-02T20:46:10.309170Z"
    }
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for record, label in zip(mnist.test.images, mnist.test.labels):\n",
    "    correct_label = np.argmax(label)\n",
    "    inputs = record * 0.99 + 0.01\n",
    "\n",
    "    outputs = nn.query(inputs)\n",
    "    label = np.argmax(outputs)\n",
    "    \n",
    "    score.append(1 if label == correct_label else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:49:57.078322Z",
     "start_time": "2017-10-02T20:49:57.065786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance = 97.550%\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance = {0:.3f}%\".format(np.array(score).mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've built our first ANN. This is a pretty small one compared to some that exist in the depths of the interwebs, but ultimately it's a start.\n",
    "\n",
    "This network's accuracy is about 97%, which is pretty bad for MNIST, but for your first network, that's pretty awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Rebuilding the ANN in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the same sort of ANN, but this time in [TensorFlow](https://www.tensorflow.org/). Ultimately, you can build your own networks and models; but one of the benefits of using a platform like TensorFlow is that it enables you to use other's models, as well as allow others to use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.212684Z",
     "start_time": "2017-10-02T20:37:23.169417Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.356039Z",
     "start_time": "2017-10-02T20:37:23.215556Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.472516Z",
     "start_time": "2017-10-02T20:37:23.364888Z"
    }
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.572798Z",
     "start_time": "2017-10-02T20:37:23.477725Z"
    }
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.665816Z",
     "start_time": "2017-10-02T20:37:23.579889Z"
    }
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.762794Z",
     "start_time": "2017-10-02T20:37:23.668902Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.886092Z",
     "start_time": "2017-10-02T20:37:23.765938Z"
    }
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.907276Z",
     "start_time": "2017-10-02T20:37:23.888682Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:24.092643Z",
     "start_time": "2017-10-02T20:37:23.912127Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.200091Z",
     "start_time": "2017-10-02T20:47:58.239030Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.396789Z",
     "start_time": "2017-10-02T20:48:16.202327Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.462904Z",
     "start_time": "2017-10-02T20:48:16.399303Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:49:59.391572Z",
     "start_time": "2017-10-02T20:49:59.363025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance = 92.230%\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance = {0:.3f}%\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
