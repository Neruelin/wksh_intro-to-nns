{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Build Your Own Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Featuring Tensorflow (TFlow).\n",
    "\n",
    "We'll be going classifying MNIST data, which is a set of ~70,000 images of handwritten digits. Bear in mind, this is a solved problem, so we're not doing anything novel.\n",
    "\n",
    "---\n",
    "**What you should leave with:**\n",
    "You should leave here with a practical understanding of how to implement an Artificial Neural Network (ANN) from nothing. The concepts don't change when you move to different domains, simply the way in which you apply them. Your understanding of the *central* concept of ANNs, **backpropagation (backprop)** should be well founded and given some more practice, you could explain this to a friend.\n",
    "\n",
    "You should also leave here with a minimal understanding TensorFlow and how using such a library can speed up your model development, as well as understanding some of it's drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "1. [Some Pre-processing](#1.-Some-Pre-processing)\n",
    "2. [Building an ANN from Sratch](#2.-Building-an-ANN-from-Sratch)\n",
    "3. [Rebuilding the ANN in TensorFlow](#3.-Rebuilding-the-ANN-in-TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Some Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:32:54.466913Z",
     "start_time": "2017-10-02T20:32:40.592018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building an ANN from Sratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build an Artificial Neural Network (ANN) from the ground-up, using raw [Python](http://python.org/), [NumPy](http://numpy.org/), and [ScipPy](http://scipy.org/).\n",
    "\n",
    "We'll build the ANN from the group up to give you the intuition behind how one would go about creating an ANN. These **_can_** run faster than the ANN built with TensorFlow, Torch, or other libraries, but these libraries introduce simplicity of building (which you'll see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### On to the workshop.\n",
    "\n",
    "Let's import our dependencies, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.709003Z",
     "start_time": "2017-10-02T20:34:42.699622Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a ANN class, called `NeuralNetwork`, this will contain two functions, and an initializer.\n",
    "\n",
    "The functions are: `train(...)` and `query(...)`. The `...` is because we don't necessarily know what we should be passing through to these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We're going to build the functions that go into the class, so we take things a step as a time, and so this commentary can be there in between. Once we've built the functions, we'll copy-paste them into the class definition and run with it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building the Initializer: `__init__(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__(...)` is almost like a constructor. Essentially, we use this to setup some instance variables that enable us to avoid passing the ANN's configuration to each function we call.\n",
    "\n",
    "This function should have a few variables that keep track of and add to the class:\n",
    "- the number of input nodes\n",
    "- the number of hidden nodes\n",
    "- the number of output nodes\n",
    "- the learning rate\n",
    "- the weights from the input to hidden layers\n",
    "- the weights from the hidden to output layers\n",
    "- the activation function (we should specify this as a separate function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.853783Z",
     "start_time": "2017-10-02T20:34:42.717344Z"
    }
   },
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def __init__(self, n_inodes, n_hnodes, n_onodes, learn_rate):\n",
    "    ## these determine the number of nodes per layer\n",
    "    self.i_cnt = n_inodes\n",
    "    self.h_cnt = n_hnodes\n",
    "    self.o_cnt = n_onodes\n",
    "    \n",
    "    ## specify the learning rate\n",
    "    self.lr = learn_rate\n",
    "    \n",
    "    \n",
    "    ## weight initialization\n",
    "    ## this can be done in one of two ways;\n",
    "    ## 1. we can randomly do so, then shift by 0.5 to 0-center our weights to introduce some negativity\n",
    "    ## 2. we can pull from a normal distribution based on some rather well established rationales\n",
    "    \n",
    "    ## going with option 1:\n",
    "    self.w_i2h = np.random.rand(self.h_cnt, self.i_cnt) - 0.5\n",
    "    self.w_h2o = np.random.rand(self.o_cnt, self.h_cnt) - 0.5\n",
    "    \n",
    "    ## going with option 2:\n",
    "    self.w_i2h = np.random.normal(0, pow(self.h_cnt, -0.5), (self.h_cnt, self.i_cnt))\n",
    "    self.w_h2o = np.random.normal(0, pow(self.o_cnt, -0.5), (self.o_cnt, self.h_cnt))\n",
    "    \n",
    "    \n",
    "    ## we can now specify the activation function, we'll do so as a lambda function\n",
    "    self.activation = lambda x: expit(x)\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building the Query Function: `query(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `query(...)` function should enable us to talk to the ANN and ask it to classify some images we hand it.\n",
    "\n",
    "We write this function before `train(...)` because it's less complex in nature because it equates to a forward pass through the network. This should help us ground the ideas that we should be implementing into `train(...)` when we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.921291Z",
     "start_time": "2017-10-02T20:34:42.857016Z"
    }
   },
   "outputs": [],
   "source": [
    "def query():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def query(self, input_list):\n",
    "    ## convert input list to np.array and transpose because of matrix mult\n",
    "    inputs = np.array(input_list, ndmin=2).T\n",
    "    \n",
    "    \n",
    "    ## propagate the input through the hidden layer\n",
    "    ### recall at X_{hidden} = W_{input_hidden} * I_{inputs}\n",
    "    hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "    ### pass `hidden_in` through the activation function to calculate the output\n",
    "    hidden_out = self.sigmoid(hidden_in)\n",
    "    \n",
    "    \n",
    "    ## propagate the hidden output through the ouput layer\n",
    "    ### recall at X_{output} = W_{hidden_output} * I_{hidden_out}\n",
    "    output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "    ### pass `output_in` through the activation function to calculate the output\n",
    "    output_out = self.sigmoid(output_in)\n",
    "    \n",
    "    return output_out\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building the Train Function: `train(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train(...)` function is how the ANN learns. We'll hand it our dataset with the labels for it to validate itself on by completing forward passes and updating the weights through backprop.\n",
    "\n",
    "We'll need to hand this function:\n",
    "- our inputs\n",
    "- our expected values\n",
    "\n",
    "Now that we've given our network data to train on, we need to implement the forward pass, followed by the backward pass. **Recall** that the backward pass involves a few stages. First, we need to calculate the output error, then distribute that error backwards across the network. This will update our weights, but the update will be moderated by the learning rate, which we specified earlier in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:42.995363Z",
     "start_time": "2017-10-02T20:34:42.925721Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def train(self, input_list, target_list):\n",
    "    ## convert input list to np.array and transpose because of matrix mult\n",
    "    inputs  = np.array(input_list, ndmin=2).T\n",
    "    targets = np.array(target_list, ndmin=2).T\n",
    "    \n",
    "    \n",
    "    hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "    hidden_out = self.activation(hidden_in)\n",
    "    \n",
    "    output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "    output_out = self.activation(output_in)\n",
    "    \n",
    "    output_err = (target - output_out)\n",
    "    \n",
    "    hidden_err = np.dot(self.w_h2o.T, output_err)\n",
    "    \n",
    "    self.w_h2o += self.lr * np.dot((output_err * output_out * (1 - output_out)), hidden_out.T)\n",
    "    self.w_i2h += self.lr * np.dot((hidden_err * hidden_out * (1 - hidden_out)), inputs.T)\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Assembling the `NeuralNetwork` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:43.171642Z",
     "start_time": "2017-10-02T20:34:43.001125Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, n_inodes, n_hnodes, n_onodes, learn_rate):\n",
    "        ## these determine the number of nodes per layer\n",
    "        self.i_cnt = n_inodes\n",
    "        self.h_cnt = n_hnodes\n",
    "        self.o_cnt = n_onodes\n",
    "\n",
    "        ## specify the learning rate\n",
    "        self.lr = learn_rate\n",
    "\n",
    "\n",
    "        ## weight initialization\n",
    "        ## this can be done in one of two ways;\n",
    "        ## 1. we can randomly do so, then shift by 0.5 to 0-center our weights to introduce some negativity\n",
    "        ## 2. we can pull from a normal distribution based on some rather well established rationales\n",
    "\n",
    "        ## going with option 1:\n",
    "        self.w_i2h = np.random.rand(self.h_cnt, self.i_cnt) - 0.5\n",
    "        self.w_h2o = np.random.rand(self.o_cnt, self.h_cnt) - 0.5\n",
    "\n",
    "        ## going with option 2:\n",
    "        self.w_i2h = np.random.normal(0, pow(self.h_cnt, -0.5), (self.h_cnt, self.i_cnt))\n",
    "        self.w_h2o = np.random.normal(0, pow(self.o_cnt, -0.5), (self.o_cnt, self.h_cnt))\n",
    "\n",
    "\n",
    "        ## we can now specify the activation function, we'll do so as a lambda function\n",
    "        self.activation = lambda x: expit(x)\n",
    "    \n",
    "    def train(self, input_list, target_list):\n",
    "        ## convert input list to np.array and transpose because of matrix mult\n",
    "        inputs  = np.array(input_list, ndmin=2).T\n",
    "        targets = np.array(target_list, ndmin=2).T\n",
    "\n",
    "\n",
    "        hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "\n",
    "        output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "        output_out = self.activation(output_in)\n",
    "\n",
    "        output_err = (targets - output_out)\n",
    "\n",
    "        hidden_err = np.dot(self.w_h2o.T, output_err)\n",
    "\n",
    "        self.w_h2o += self.lr * np.dot((output_err * output_out * (1 - output_out)), np.transpose(hidden_out))\n",
    "        self.w_i2h += self.lr * np.dot((hidden_err * hidden_out * (1 - hidden_out)), np.transpose(inputs))\n",
    "        \n",
    "    def query(self, input_list):\n",
    "        ## convert input list to np.array and transpose because of matrix mult\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "\n",
    "        \n",
    "        ## propagate the input through the hidden layer\n",
    "        ### recall at X_{hidden} = W_{input_hidden} * I_{inputs}\n",
    "        hidden_in  = np.dot(self.w_i2h, inputs)\n",
    "        ### pass `hidden_in` through the activation function to calculate the output\n",
    "        hidden_out = self.activation(hidden_in)\n",
    "\n",
    "\n",
    "        ## propagate the hidden output through the ouput layer\n",
    "        ### recall at X_{output} = W_{hidden_output} * I_{hidden_out}\n",
    "        output_in  = np.dot(self.w_h2o, hidden_out)\n",
    "        ### pass `output_in` through the activation function to calculate the output\n",
    "        output_out = self.activation(output_in)\n",
    "\n",
    "        return output_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training the Network on MNIST\n",
    "\n",
    "Now we'll move on the training the network on MNIST, but to do so, we need to specify some parameters.\n",
    "\n",
    "**Recall** that these images are `28x28` pixel images, which results in a total of `784` inputs. We ultimately need to classify these images into `10` classes, as we're analyzing the numbers `0-9`. The hidden layers is rather arbitrary in size, so we can use just about any amount of hidden layers we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:34:43.269719Z",
     "start_time": "2017-10-02T20:34:43.174403Z"
    }
   },
   "outputs": [],
   "source": [
    "n_inodes = 1\n",
    "n_hnodes = 1\n",
    "n_onodes = 1\n",
    "\n",
    "learn_rt = 1\n",
    "\n",
    "nn = NeuralNetwork(n_inodes, n_hnodes, n_onodes, learn_rt)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "n_inodes = 784\n",
    "n_hnodes = 200\n",
    "n_onodes =  10\n",
    "\n",
    "learn_rt = 0.1\n",
    "\n",
    "nn = NeuralNetwork(n_inodes, n_hnodes, n_onodes, learn_rt)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've initialized the ANN, so now we need to actually execute the training of it. We'll train over `N` *epochs*, which are essentially just the number of times we go over the data to see if we can continue to refine the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:42:42.093456Z",
     "start_time": "2017-10-02T20:40:03.720358Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    for record, label in zip(mnist.train.images, mnist.train.labels):\n",
    "        record[record == float(0)] = 0.01\n",
    "        label[label == float(0)] = 0.01\n",
    "        nn.train(record, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the Network\n",
    "\n",
    "We've built and trained the ANN, now; so the next step we should take is to test our accuracy to see if our model has actually learned, well, from the data we've given it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:46:10.950397Z",
     "start_time": "2017-10-02T20:46:10.309170Z"
    }
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for record, label in zip(mnist.test.images, mnist.test.labels):\n",
    "    correct_label = np.argmax(label)\n",
    "    inputs = record * 0.99 + 0.01\n",
    "\n",
    "    outputs = nn.query(inputs)\n",
    "    label = np.argmax(outputs)\n",
    "    \n",
    "    score.append(1 if label == correct_label else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:49:57.078322Z",
     "start_time": "2017-10-02T20:49:57.065786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance = 97.350%\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance = {0:.3f}%\".format(np.array(score).mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've built our first ANN. This is a pretty small one compared to some that exist in the depths of the interwebs, but ultimately it's a start.\n",
    "\n",
    "This network's accuracy is about 97%, which is pretty bad for MNIST, but for your first network, that's pretty awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Rebuilding the ANN in TensorFlow\n",
    "\n",
    "*NOTE:* This is a walk-through of the official tutorial [here](https://www.tensorflow.org/get_started/mnist/beginners)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the same sort of ANN, but this time in [TensorFlow](https://www.tensorflow.org/). Ultimately, you can build your own networks and models; but one of the benefits of using a platform like TensorFlow is that it enables you to use other's models, as well as allow others to use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.212684Z",
     "start_time": "2017-10-02T20:37:23.169417Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, a placeholder is a promise that we'll provide a value later; it's akin to declaring a variable, but not initializing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.356039Z",
     "start_time": "2017-10-02T20:37:23.215556Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are what you're used to from programming; in TensorFlow, these are considered \"trainable parameters\" that get added to your Graph. They're akin to declaring and initializing the variable. They have a `dtype` and initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.472516Z",
     "start_time": "2017-10-02T20:37:23.364888Z"
    }
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is one of many different activation functions. Softmax is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax because our ultimate goal is to assign probabilities to our input that it belongs to one of the 10 classes of numbers. It will give us a list of results from \\[0, 1\\] which all sum to 1, as well.\n",
    "\n",
    "One of the reasons for using softmax is that it allows us to assign probabilities between 0 and 1 for a collection of data that we want. So, if we had 4 outputs to choose from, softmax could give an output like \\[0.1, 0.2, 0.4, 0.3\\] and our classifier would choose whatever class 2 (CS#2) represents.\n",
    "\n",
    "Softmax is composed of two steps:\n",
    "1. Add up all the evidence that our input belongs to a specific class.\n",
    "2. Cover that evidence into a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softmax(x) = normalize(exp(x)) = \\frac{exp(x_i)}{\\sum_{j} exp(x_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.572798Z",
     "start_time": "2017-10-02T20:37:23.477725Z"
    }
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.665816Z",
     "start_time": "2017-10-02T20:37:23.579889Z"
    }
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need a loss function, like SSE in the ANN. ross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning.\n",
    "\n",
    "In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.762794Z",
     "start_time": "2017-10-02T20:37:23.668902Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.886092Z",
     "start_time": "2017-10-02T20:37:23.765938Z"
    }
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we actually interact with the network we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:23.907276Z",
     "start_time": "2017-10-02T20:37:23.888682Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:24.092643Z",
     "start_time": "2017-10-02T20:37:23.912127Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.200091Z",
     "start_time": "2017-10-02T20:47:58.239030Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.396789Z",
     "start_time": "2017-10-02T20:48:16.202327Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:48:16.462904Z",
     "start_time": "2017-10-02T20:48:16.399303Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:49:59.391572Z",
     "start_time": "2017-10-02T20:49:59.363025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance = 91.510%\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance = {0:.3f}%\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've rebuilt an equivalent ANN in TensorFlow. This network's accuracy is about 91%, which is pretty bad for MNIST, but for a raw neural network, it's still pretty awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "print(\"Performance = {0:.3f}%\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
